{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next text, perform the following actions\n",
    "text = \"The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.\"\n",
    "\n",
    "# (1 point) 1 - Use NLTK to split the sentences \n",
    "\n",
    "# (2 points) 2 - Convert with regex the acronym U.S.A. to USA, the number 1.9m to 190 centimeters or any other number of a height like that (e.g. 1.75m to 175 centimeters), and \"$5.5 billion\" to five point five billion. \n",
    "\n",
    "# (1 point) 3 - Convert to lowercase except the proper nouns that must keep the original case. For the multiword proper names convert them to an unique word joining the two word with underscoere (Juan Fern치ndez -> Juan_Fern치ndez).\n",
    "\n",
    "# (1 point) 4 - Tokenize the text (use the tool you prefer). \n",
    "\n",
    "# (1 point) 5 - Remove the stopwords (use the tool you prefer). \n",
    "\n",
    "# (1 point) 6 - Create bigrams with pure python.\n",
    "\n",
    "# (2 point) 7 - Create a language model that predict the next word using bigrams. Please explain in the code how you made the calculations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from num2words import num2words\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) 1 - Use NLTK to split the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old.', 'Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) 2 - Convert with regex the acronym U.S.A. to USA, the number 1.9m to 190 centimeters or any other number of a height like that (e.g. 1.75m to 175 centimeters), and \"$5.5 billion\" to five point five billion. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the USA, Donald Trump, is 190 centimeters high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'U\\.S\\.A\\.', 'USA', text)\n",
    "\n",
    "text = re.sub(r'(\\d+\\.\\d+)m', lambda x: f\"{int(float(x.group(1)) * 100)} centimeters\", text)\n",
    "\n",
    "def decimal_to_words(match):\n",
    "    number = match.group(1)  # Extract the decimal number (e.g., \"5.5\")\n",
    "    integer_part, decimal_part = number.split(\".\")  # Split into whole and decimal parts\n",
    "    return f\"{num2words(int(integer_part))} point {num2words(int(decimal_part))} billion\"\n",
    "\n",
    "text = re.sub(r'\\$(\\d+\\.\\d+)\\sbillion', decimal_to_words, text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) 3 - Convert to lowercase except the proper nouns that must keep the original case. For the multiword proper names convert them to an unique word joining the two word with underscoere (Juan Fern치ndez -> Juan_Fern치ndez).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the president of the USA, Donald_Trump, is 190 centimeters high and 78 years old. Forbes_Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    proper_nouns = [\"USA\", \"Donald Trump\", \"Forbes Magazine\", \"mid-February\"]\n",
    "    \n",
    "    for proper in proper_nouns:\n",
    "        text = re.sub(r'\\b' + re.escape(proper) + r'\\b', proper.replace(\" \", \"_\"), text)\n",
    "\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if \"_\" in word or word.isupper() or word == \"mid-February\":  \n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            processed_words.append(word.casefold())\n",
    "\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "processed_text = process_text(text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) 4 - Tokenize the text (use the tool you prefer). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the president of the USA, Donald_Trump, is 190 centimeters high and 78 years old.', 'Forbes_Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.']\n",
      "['the', 'president', 'of', 'the', 'USA', ',', 'Donald_Trump', ',', 'is', '190', 'centimeters', 'high', 'and', '78', 'years', 'old', '.', 'Forbes_Magazine', 'has', 'assessed', 'his', 'wealth', ',', 'currently', 'estimating', 'it', 'at', 'five', 'point', 'five', 'billion', 'as', 'of', 'mid-February', '2025', '.']\n"
     ]
    }
   ],
   "source": [
    "processed_words = nltk.word_tokenize(processed_text)\n",
    "processed_sentences = nltk.sent_tokenize(processed_text)\n",
    "print(processed_sentences)\n",
    "print(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) 5 - Remove the stopwords (use the tool you prefer). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president', 'USA', ',', 'Donald_Trump', ',', '190', 'centimeters', 'high', '78', 'years', 'old', '.', 'Forbes_Magazine', 'assessed', 'wealth', ',', 'currently', 'estimating', 'five', 'point', 'five', 'billion', 'mid-February', '2025', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "def removestopwords(tokens):\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))  # Get the stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_words = removestopwords(processed_words)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) 6 - Create bigrams with pure python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'president'), ('president', 'of'), ('of', 'the'), ('the', 'USA'), ('USA', ','), (',', 'Donald_Trump'), ('Donald_Trump', ','), (',', 'is'), ('is', '190'), ('190', 'centimeters'), ('centimeters', 'high'), ('high', 'and'), ('and', '78'), ('78', 'years'), ('years', 'old'), ('old', '.'), ('.', 'Forbes_Magazine'), ('Forbes_Magazine', 'has'), ('has', 'assessed'), ('assessed', 'his'), ('his', 'wealth'), ('wealth', ','), (',', 'currently'), ('currently', 'estimating'), ('estimating', 'it'), ('it', 'at'), ('at', 'five'), ('five', 'point'), ('point', 'five'), ('five', 'billion'), ('billion', 'as'), ('as', 'of'), ('of', 'mid-February'), ('mid-February', '2025'), ('2025', '.')]\n"
     ]
    }
   ],
   "source": [
    "def generate_bigrams(tokens):\n",
    "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "    return bigrams\n",
    "\n",
    "bigrams = generate_bigrams(processed_words)\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 point) 7 - Create a language model that predict the next word using bigrams. Please explain in the code how you made the calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next word after 'Donald_Trump' is: ,\n"
     ]
    }
   ],
   "source": [
    "def train_bigram_model(tokens):\n",
    "    # Generate bigrams\n",
    "    bigrams = generate_bigrams(tokens)\n",
    "    \n",
    "    # Frequency of each word pair (bigram) and individual word counts\n",
    "    bigram_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    # Count the bigrams and individual words\n",
    "    for w1, w2 in bigrams:\n",
    "        bigram_counts[(w1, w2)] += 1\n",
    "        word_counts[w1] += 1\n",
    "\n",
    "    return bigram_counts, word_counts\n",
    "\n",
    "def predict_next_word(word, bigram_counts, word_counts):\n",
    "    # Calculate the probability of each possible next word given the current word\n",
    "    possible_bigrams = [(w1, w2) for (w1, w2) in bigram_counts if w1 == word]\n",
    "    \n",
    "    if not possible_bigrams:\n",
    "        return None  # No possible next word found\n",
    "    \n",
    "    # Calculate the probability for each next word\n",
    "    next_word_probabilities = {}\n",
    "    for w1, w2 in possible_bigrams:\n",
    "        next_word_probabilities[w2] = bigram_counts[(w1, w2)] / word_counts[w1]\n",
    "\n",
    "    # Return the next word with the highest probability\n",
    "    predicted_word = max(next_word_probabilities, key=next_word_probabilities.get)\n",
    "    return predicted_word\n",
    "\n",
    "# Train the model\n",
    "bigram_counts, word_counts = train_bigram_model(processed_words)\n",
    "\n",
    "# Predict the next word after a given word\n",
    "current_word = input(\"Enter a word to predict the next word: \")\n",
    "predicted_word = predict_next_word(current_word, bigram_counts, word_counts)\n",
    "\n",
    "if predicted_word:\n",
    "    print(f\"The predicted next word after '{current_word}' is: {predicted_word}\")\n",
    "else:\n",
    "    print(f\"No prediction found for the word '{current_word}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
